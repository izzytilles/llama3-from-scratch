{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama3 From Scratch \n",
    "By: Isabel Tilles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Tokenizer sourced from tiktoken  \n",
    "Using Llama-3.2-1B-Instruct-QLORA_INT4_EO8 from Meta due to space restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have read access to the file.\n",
      "YAYYYY.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "tokenizer_path = \"/root/.llama/checkpoints/Llama3.2-1B-Instruct-int4-qlora-eo8/\"\n",
    "path = Path(tokenizer_path)\n",
    "path2 = Path(tokenizer_path).resolve()\n",
    "\n",
    "if os.access(path, os.R_OK):\n",
    "    print(\"You have read access to the file.\")\n",
    "else:\n",
    "    print(\"You do not have read access to the file.\")\n",
    "\n",
    "\n",
    "if path.exists():\n",
    "    print(\"YAYYYY.\")\n",
    "else:\n",
    "    print(path2)\n",
    "\n",
    "# special_tokens = [\n",
    "#             \"<|begin_of_text|>\",\n",
    "#             \"<|end_of_text|>\",\n",
    "#             \"<|reserved_special_token_0|>\",\n",
    "#             \"<|reserved_special_token_1|>\",\n",
    "#             \"<|reserved_special_token_2|>\",\n",
    "#             \"<|reserved_special_token_3|>\",\n",
    "#             \"<|start_header_id|>\",\n",
    "#             \"<|end_header_id|>\",\n",
    "#             \"<|reserved_special_token_4|>\",\n",
    "#             \"<|eot_id|>\",  # end of turn\n",
    "#         ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "# mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "# tokenizer = tiktoken.Encoding(\n",
    "#     name=Path(tokenizer_path).name,\n",
    "#     pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "#     mergeable_ranks=mergeable_ranks,\n",
    "#     special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    "# )\n",
    "\n",
    "# tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Meta-Llama-3.2-1B-Instruct-QLORA_INT4_EO8/consolidated.00.pth\")\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Meta-Llama-3.2-1B-Instruct-QLORA_INT4_EO8/params.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use this config to infer details about the model like\n",
    "1. the model has XXXX transformer layers\n",
    "2. each multi-head attention block has XXXX heads\n",
    "3. the vocab size is XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"dim\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "n_heads = config[\"n_heads\"]\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to tokens\n",
    "Here we use tiktoken (an openai library) as the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "tokens = [128000] + tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "tokens = torch.tensor(tokens)\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]\n",
    "print(prompt_split_as_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting tokens to their embedding\n",
    "IM SORRY but this is the only part of the codebase where i use an inbuilt neural network module\n",
    "anyway, so our [XXXX] tokens are now [XXXX], i.e. 17 embeddings (one for each token) of length 4096\n",
    "\n",
    "note: keep track of the shapes, it makes it much easier to understand everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
    "embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
    "token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the embedding using rms normalization\n",
    "please, note after this step the shapes dont change, the values are just normalized\n",
    "things to keep in mind, we need a norm_eps (from config) because we dont want to accidently set rms to 0 and divide by 0\n",
    "here is the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rms_norm(tensor, norm_weights):\n",
    "#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5\n",
    "#     return tensor * (norm_weights / rms)\n",
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the first first layer of the transformer\n",
    "## normalization\n",
    "you will see me accessing layer.0 from the model dict (this is the first layer)\n",
    "anyway, so after normalizing our shapes are still [XXXX] same as embedding but normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = rms_norm(token_embeddings_unnormalized, model[\"layers.0.attention_norm.weight\"])\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention implemented from scratch\n",
    "let's load the attention heads of the first layer of the transformer\n",
    "\n",
    "> when we load the query, key, value and output vectors from the model we notice the shapes to be [XXXX], [XXXX], [XXXX], [XXXX]\n",
    "> at first glance this is weird because ideally we want each q,k,v and o for each head individually\n",
    "> the authors of the code bundled them togeather because its easy it helps parallize attention head multiplication.\n",
    "> im going to unwrap everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    model[\"layers.0.attention.wq.weight\"].shape,\n",
    "    model[\"layers.0.attention.wk.weight\"].shape,\n",
    "    model[\"layers.0.attention.wv.weight\"].shape,\n",
    "    model[\"layers.0.attention.wo.weight\"].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unwrapping query\n",
    "in the next section we will unwrap the queries from multiple attention heads, the resulting shape is [XXXX]\n",
    "\n",
    "here, 32 is the number of attention heads in llama3, 128 is the size of the query vector and 4096 is the size of the token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer0 = model[\"layers.0.attention.wq.weight\"]\n",
    "head_dim = q_layer0.shape[0] // n_heads\n",
    "q_layer0 = q_layer0.view(n_heads, head_dim, dim)\n",
    "q_layer0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm going to implement the first head of the first layer\n",
    "here i access the query weight matrix first head of the first layer, the size of this query weight matrix is [XXXX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer0_head0 = q_layer0[0]\n",
    "q_layer0_head0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now multiply the query weights with the token embedding, to recive a query for the token\n",
    "here you can see the resulting shape is [XXXX], this is because we have XXXX tokens and for each token there is a XXXX length query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)\n",
    "q_per_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positioning encoding\n",
    "we are now at a stage where we have a query vector for each token in our prompt, but if you think about it -- the indivitually query vector has no idea about the position in the prompt.\n",
    "\n",
    "query: \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "\n",
    "in our prompt we have used \"the\" three times, we need the query vectors of all 3 \"the\" tokens to have different query vectors (each of size [1x128]) based on their positions in the query. we perform these rotations using RoPE (rotory positional embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "q_per_token_split_into_pairs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the above step, we split the query vectors into pairs, we apply a rotational angle shift to each pair!\n",
    "\n",
    "we now have a vector of size [XXXX], this is the XXXX length queries split into XXXX pairs for each token in the prompt! each of those XXXX pairs will be rotated by m*(theta) where m is the position of the token for which we are rotating the query!  \n",
    "\n",
    "### Using dot product of complex numbers to rotate a vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
    "zero_to_one_split_into_64_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_for_each_token = torch.outer(torch.arange(17), freqs)\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "freqs_cis.shape\n",
    "\n",
    "# viewing tjhe third row of freqs_cis\n",
    "value = freqs_cis[3]\n",
    "plt.figure()\n",
    "for i, element in enumerate(value[:17]):\n",
    "    plt.plot([0, element.real], [0, element.imag], color='blue', linewidth=1, label=f\"Index: {i}\")\n",
    "    plt.annotate(f\"{i}\", xy=(element.real, element.imag), color='red')\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Imaginary')\n",
    "plt.title('Plot of one row of freqs_cis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a complex number (the angle change vector) for every token's query element\n",
    "we can convert our queries (the one we split into pairs) as complex numbers and then dot product to rotate the query based on the position\n",
    "honeslty this is beautiful to think about :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "q_per_token_as_complex_numbers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis\n",
    "q_per_token_as_complex_numbers_rotated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after rotated vector is obtained\n",
    "we can get back our the queries as pairs by viewing the complex numbers as real numbers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
